#!/usr/bin/env python3
"""
æ¶ˆæ¯å¤„ç†å™¨
å¤„ç†æ¥è‡ª Electron çš„æ¶ˆæ¯å¹¶è¿”å›å“åº”
"""
import asyncio
import json
import logging
import time
from typing import Any, Dict, Optional

from model_router import model_router, ModelConfig, ModelProvider
from ollama_client import check_ollama_status, get_ollama_models, get_ollama_client
from ask import AskHandler

logger = logging.getLogger(__name__)


class MessageHandler:
    """æ¶ˆæ¯å¤„ç†å™¨"""

    def __init__(self, send_callback=None):
        """
        åˆå§‹åŒ–æ¶ˆæ¯å¤„ç†å™¨

        Args:
            send_callback: å‘é€æ¶ˆæ¯çš„å¼‚æ­¥å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼æ¶ˆæ¯
        """
        self.send_callback = send_callback
        # åˆå§‹åŒ– Ask æ¨¡å—
        self.ask_handler = AskHandler(send_callback=send_callback)
        
        # è®¾ç½® AskHandler å¼•ç”¨åˆ° Todo å·¥å…·æ¨¡å—
        from agent.todo_tool import set_ask_handler
        set_ask_handler(self.ask_handler)
        
        self.handlers = {
            "ping": self._handle_ping,
            "chat_message": self._handle_chat_message,
            "agent_chat": self._handle_agent_chat,  # Agent èŠå¤©æ¨¡å¼
            "system_status": self._handle_system_status,
            "model_register": self._handle_model_register,
            "model_unregister": self._handle_model_unregister,
            "model_test": self._handle_model_test,
            "model_config_sync": self._handle_model_config_sync,
            "connection_ack": self._handle_connection_ack,
            # Ollama ç›¸å…³
            "ollama_status": self._handle_ollama_status,
            "ollama_models": self._handle_ollama_models,
            "ollama_test": self._handle_ollama_test,
            # Skills æŠ€èƒ½ç›¸å…³
            "skill_list": self._handle_skill_list,
            "skill_execute": self._handle_skill_execute,
            "skill_reload": self._handle_skill_reload,
            # Knowledge çŸ¥è¯†åº“ç›¸å…³
            "knowledge_create": self._handle_knowledge_create,
            "knowledge_delete": self._handle_knowledge_delete,
            "knowledge_list": self._handle_knowledge_list,
            "knowledge_get": self._handle_knowledge_list,  # å¤ç”¨ list å¤„ç†
            "knowledge_add_document": self._handle_knowledge_add_document,
            "knowledge_remove_document": self._handle_knowledge_list_documents,  # å¤ç”¨
            "knowledge_search": self._handle_knowledge_search,
            "knowledge_list_documents": self._handle_knowledge_list_documents,
            # Memory è®°å¿†ç›¸å…³
            "memory_generate_summary": self._handle_memory_generate_summary,
            "memory_extract": self._handle_memory_extract,
            # Web Crawl ç½‘é¡µé‡‡é›†ç›¸å…³
            "web_crawl": self._handle_web_crawl,
            "web_fetch": self._handle_web_fetch,
            # Ask é€šç”¨äº¤äº’æ¨¡å—
            "ask_response": self._handle_ask_response,
        }
        # ä¼šè¯å­˜å‚¨ï¼ˆåç»­å¯æ›¿æ¢ä¸ºæŒä¹…åŒ–å­˜å‚¨ï¼‰
        self.conversations: Dict[str, list] = {}

    async def process(self, message: dict) -> Optional[dict]:
        """å¤„ç†æ¶ˆæ¯"""
        msg_type = message.get("type")

        if not msg_type:
            return self._error_response("æ— æ•ˆçš„æ¶ˆæ¯ç±»å‹", message.get("id"))

        handler = self.handlers.get(msg_type)
        if not handler:
            logger.warning(f"æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹: {msg_type}")
            return self._error_response(f"æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹: {msg_type}", message.get("id"))

        try:
            return await handler(message)
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯é”™è¯¯: {e}")
            return self._error_response(str(e), message.get("id"))

    async def _handle_ping(self, message: dict) -> dict:
        """å¤„ç†å¿ƒè·³æ¶ˆæ¯"""
        return {
            "type": "pong",
            "id": message.get("id"),
            "timestamp": int(time.time() * 1000)
        }

    async def _handle_connection_ack(self, message: dict) -> Optional[dict]:
        """å¤„ç†è¿æ¥ç¡®è®¤æ¶ˆæ¯"""
        logger.debug(f"æ”¶åˆ°è¿æ¥ç¡®è®¤: {message.get('clientId')}")
        return None  # ä¸éœ€è¦å“åº”

    async def _handle_chat_message(self, message: dict) -> dict:
        """
        å¤„ç†èŠå¤©æ¶ˆæ¯

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. æ™®é€šèŠå¤©ï¼šç›´æ¥è°ƒç”¨ LLM
        2. å·¥å…·å¢å¼ºèŠå¤©ï¼šä½¿ç”¨ Function Callingï¼Œè®© LLM å¯ä»¥è°ƒç”¨å·¥å…·
        """
        content = message.get("content", "")
        conversation_id = message.get("conversationId")
        model_id = message.get("modelId")  # å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹
        stream = message.get("stream", False)  # æ˜¯å¦æµå¼è¾“å‡º
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å†å²è®°å½•ï¼ˆæ»‘åŠ¨çª—å£ç­–ç•¥ï¼‰
        incoming_history = message.get("history", [])
        # æ˜¯å¦å¯ç”¨å·¥å…·ï¼ˆFunction Callingï¼‰
        use_tools = message.get("useTools", False)  # é»˜è®¤ä¸å¯ç”¨

        logger.info(f"æ”¶åˆ°èŠå¤©æ¶ˆæ¯: {content[:50]}... (useTools={use_tools})")

        # å¦‚æœæ²¡æœ‰ä¼ å…¥å†å²è®°å½•ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„ä¼šè¯å†å²ï¼ˆå…¼å®¹æ—§å®¢æˆ·ç«¯ï¼‰
        if not incoming_history and conversation_id:
            if conversation_id not in self.conversations:
                self.conversations[conversation_id] = []
            self.conversations[conversation_id].append({
                "role": "user",
                "content": content,
                "timestamp": int(time.time() * 1000)
            })

        # ä½¿ç”¨æ¨¡å‹è·¯ç”±å™¨å¤„ç†
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []

            if incoming_history:
                # ä½¿ç”¨ä¼ å…¥çš„å†å²è®°å½•
                for msg in incoming_history:
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
            elif conversation_id and conversation_id in self.conversations:
                # å…¼å®¹æ—§å®¢æˆ·ç«¯ï¼šä½¿ç”¨å†…å­˜ä¸­çš„å†å²æ¶ˆæ¯
                for msg in self.conversations[conversation_id][:-1]:  # ä¸åŒ…æ‹¬åˆšæ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": content})

            # å¦‚æœå¯ç”¨å·¥å…·ï¼Œä½¿ç”¨ Function Calling
            if use_tools:
                return await self._handle_chat_with_tools(
                    message, messages, model_id, conversation_id
                )

            # è°ƒç”¨æ¨¡å‹
            if stream:
                # æµå¼å“åº”é€šè¿‡ WebSocket å‘é€å¤šä¸ªæ¶ˆæ¯
                return await self._handle_stream_chat(message, messages, model_id)
            else:
                response_content = await model_router.chat_async(
                    messages=messages,
                    model_id=model_id
                )

                # å­˜å‚¨å“åº”ï¼ˆä»…åœ¨æ²¡æœ‰ä¼ å…¥å†å²æ—¶ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼‰
                if not incoming_history and conversation_id:
                    self.conversations[conversation_id].append({
                        "role": "assistant",
                        "content": response_content,
                        "timestamp": int(time.time() * 1000)
                    })

                return {
                    "type": "chat_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "content": response_content,
                    "conversationId": conversation_id,
                    "success": True
                }

        except ValueError as e:
            # æ¨¡å‹æœªæ³¨å†Œï¼Œå›é€€åˆ°æ¨¡æ‹Ÿå“åº”
            logger.warning(f"æ¨¡å‹æœªæ³¨å†Œï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {e}")
            response_content = await self._generate_mock_response(content)

            if not incoming_history and conversation_id:
                self.conversations[conversation_id].append({
                    "role": "assistant",
                    "content": response_content,
                    "timestamp": int(time.time() * 1000)
                })

            return {
                "type": "chat_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "content": response_content,
                "conversationId": conversation_id,
                "success": True,
                "fallback": True  # æ ‡è®°ä¸ºå›é€€å“åº”
            }
        except Exception as e:
            logger.error(f"å¤„ç†èŠå¤©æ¶ˆæ¯é”™è¯¯: {e}")
            return self._error_response(str(e), message.get("id"))

    async def _handle_chat_with_tools(
        self,
        message: dict,
        messages: list,
        model_id: Optional[int],
        conversation_id: str
    ) -> dict:
        """
        ä½¿ç”¨ Function Calling çš„èŠå¤©

        è®© LLM å¯ä»¥è°ƒç”¨å·¥å…·è·å–ä¿¡æ¯ï¼Œç„¶åç”Ÿæˆå›ç­”ã€‚
        """
        from agent.tools import global_tool_registry

        # è·å–æ‰€æœ‰å·¥å…·çš„ OpenAI æ ¼å¼
        tools = global_tool_registry.get_openai_tools()
        logger.info(f"[FunctionCalling] å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")

        # å®šä¹‰å·¥å…·æ‰§è¡Œå™¨
        async def tool_executor(tool_name: str, tool_args: dict) -> str:
            """æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ"""
            try:
                result = global_tool_registry.execute_tool(
                    tool_name, tool_args)
                logger.info(f"[FunctionCalling] å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                return result
            except Exception as e:
                error_msg = f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}"
                logger.error(error_msg)
                return error_msg

        try:
            # ä½¿ç”¨æ”¯æŒå·¥å…·çš„èŠå¤©æ–¹æ³•
            response_content = await model_router.chat_with_tools(
                messages=messages,
                model_id=model_id,
                tools=tools,
                tool_executor=tool_executor
            )

            # å­˜å‚¨å“åº”
            if conversation_id and conversation_id in self.conversations:
                self.conversations[conversation_id].append({
                    "role": "assistant",
                    "content": response_content,
                    "timestamp": int(time.time() * 1000)
                })

            return {
                "type": "chat_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "content": response_content,
                "conversationId": conversation_id,
                "success": True,
                "usedTools": True  # æ ‡è®°ä½¿ç”¨äº†å·¥å…·
            }

        except Exception as e:
            logger.error(f"[FunctionCalling] å·¥å…·èŠå¤©é”™è¯¯: {e}")
            # é™çº§åˆ°æ™®é€šèŠå¤©
            response_content = await model_router.chat_async(
                messages=messages,
                model_id=model_id
            )
            return {
                "type": "chat_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "content": response_content,
                "conversationId": conversation_id,
                "success": True,
                "fallback": True
            }

    async def _handle_stream_chat(
        self, message: dict, messages: list, model_id: Optional[int]
    ) -> dict:
        """å¤„ç†æµå¼èŠå¤©"""
        conversation_id = message.get("conversationId")
        msg_id = message.get("id")
        # æ˜¯å¦ä½¿ç”¨ä¼ å…¥çš„å†å²è®°å½•
        incoming_history = message.get("history", [])

        # å‘é€æµå¼å¼€å§‹æ¶ˆæ¯
        if self.send_callback:
            await self.send_callback({
                "type": "chat_stream_start",
                "id": msg_id,
                "timestamp": int(time.time() * 1000),
                "conversationId": conversation_id,
                "modelId": model_id,
            })

        full_content = ""
        chunk_index = 0

        try:
            # æµå¼è°ƒç”¨æ¨¡å‹
            async for chunk in model_router.chat_stream_async(messages, model_id):
                full_content += chunk
                chunk_index += 1

                # å‘é€æµå¼å†…å®¹å—
                if self.send_callback:
                    await self.send_callback({
                        "type": "chat_stream_chunk",
                        "id": f"{msg_id}_chunk_{chunk_index}",
                        "timestamp": int(time.time() * 1000),
                        "conversationId": conversation_id,
                        "content": chunk,
                        "chunkIndex": chunk_index,
                    })

            # å­˜å‚¨å®Œæ•´å“åº”ï¼ˆä»…åœ¨æ²¡æœ‰ä¼ å…¥å†å²æ—¶ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼‰
            if not incoming_history and conversation_id:
                if conversation_id not in self.conversations:
                    self.conversations[conversation_id] = []
                self.conversations[conversation_id].append({
                    "role": "assistant",
                    "content": full_content,
                    "timestamp": int(time.time() * 1000)
                })

            # å‘é€æµå¼ç»“æŸæ¶ˆæ¯
            if self.send_callback:
                await self.send_callback({
                    "type": "chat_stream_end",
                    "id": f"{msg_id}_end",
                    "timestamp": int(time.time() * 1000),
                    "conversationId": conversation_id,
                    "fullContent": full_content,
                })

            return None  # ä¸è¿”å›å“åº”ï¼Œå·²é€šè¿‡æµå¼æ¶ˆæ¯å‘é€

        except Exception as e:
            logger.error(f"æµå¼èŠå¤©é”™è¯¯: {e}")
            # å‘é€é”™è¯¯æ¶ˆæ¯
            if self.send_callback:
                await self.send_callback({
                    "type": "chat_error",
                    "id": msg_id,
                    "timestamp": int(time.time() * 1000),
                    "error": str(e),
                    "conversationId": conversation_id,
                })
            return None

    async def _handle_agent_chat(self, message: dict) -> Optional[dict]:
        """
        å¤„ç† Agent èŠå¤©æ¶ˆæ¯

        Agent æ¨¡å¼ä½¿ç”¨ ReAct (Reasoning + Acting) å¾ªç¯æˆ– Deep Agentsï¼š
        1. æ€è€ƒï¼šåˆ†æç”¨æˆ·é—®é¢˜ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        2. è¡ŒåŠ¨ï¼šè°ƒç”¨å·¥å…·æˆ–ç»™å‡ºç­”æ¡ˆ
        3. è§‚å¯Ÿï¼šæŸ¥çœ‹å·¥å…·ç»“æœï¼Œç»§ç»­æ€è€ƒ

        Deep Agents æä¾›é¢å¤–èƒ½åŠ›ï¼š
        - ä»»åŠ¡è§„åˆ’ä¸åˆ†è§£ (write_todos)
        - ä¸Šä¸‹æ–‡ç®¡ç† (æ–‡ä»¶ç³»ç»Ÿå·¥å…·)
        - å­æ™ºèƒ½ä½“ç”Ÿæˆ (task å·¥å…·)
        - é•¿æœŸè®°å¿† (Memory Store)

        æ¯ä¸€æ­¥éƒ½ä¼šé€šè¿‡ WebSocket å‘é€ agent_step æ¶ˆæ¯ç»™å‰ç«¯ï¼Œ
        è®©ç”¨æˆ·çœ‹åˆ° Agent çš„æ€è€ƒè¿‡ç¨‹ã€‚
        """
        content = message.get("content", "")
        conversation_id = message.get("conversationId", "")
        model_id = message.get("modelId")  # å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹
        incoming_history = message.get("history", [])  # å†å²æ¶ˆæ¯
        knowledge_id = message.get("knowledgeId")  # çŸ¥è¯†åº“ IDï¼ˆå¯é€‰ï¼‰
        knowledge_metadata = message.get("knowledgeMetadata")  # çŸ¥è¯†åº“å…ƒæ•°æ®
        use_deep_agent = message.get("useDeepAgent", True)  # æ˜¯å¦ä½¿ç”¨ Deep Agent
        attachments = message.get("attachments", [])  # é™„ä»¶åˆ—è¡¨
        msg_id = message.get("id")

        logger.info(f"[Agent] æ”¶åˆ°æ¶ˆæ¯: {content[:50]}...")
        logger.info(f"[Agent] çŸ¥è¯†åº“å…ƒæ•°æ®: {knowledge_metadata}")
        logger.info(f"[Agent] ä½¿ç”¨ Deep Agent: {use_deep_agent}")
        logger.info(f"[Agent] é™„ä»¶æ•°é‡: {len(attachments)}")
        if attachments:
            for att in attachments:
                logger.info(
                    f"[Agent] é™„ä»¶: {att.get('name')} | è·¯å¾„: {att.get('path')} | ç±»å‹: {att.get('type')}")

        try:
            from langchain_core.messages import HumanMessage
            from agent.knowledge_tool import KnowledgeRetrieverTool
            from agent.tools import global_tool_registry

            # è®¾ç½®çŸ¥è¯†åº“å…ƒæ•°æ®ï¼ˆç”¨äºæ™ºèƒ½åŒ¹é…ï¼‰
            if knowledge_metadata:
                KnowledgeRetrieverTool.set_knowledge_metadata(
                    knowledge_metadata)
                logger.info(
                    f"[Agent] å·²è®¾ç½®çŸ¥è¯†åº“å…ƒæ•°æ®: {list(knowledge_metadata.keys())}")

            # å¦‚æœæŒ‡å®šäº†çŸ¥è¯†åº“ï¼Œè®¾ç½®é»˜è®¤çŸ¥è¯†åº“
            if knowledge_id:
                KnowledgeRetrieverTool.set_default_knowledge(knowledge_id)
                logger.info(f"[Agent] å·²è®¾ç½®é»˜è®¤çŸ¥è¯†åº“: {knowledge_id}")
            else:
                KnowledgeRetrieverTool.set_default_knowledge(None)

            # å‘é€æµå¼å¼€å§‹æ¶ˆæ¯ï¼ˆè®©å‰ç«¯çŸ¥é“ conversationIdï¼‰
            if self.send_callback:
                await self.send_callback({
                    "type": "chat_stream_start",
                    "id": msg_id,
                    "timestamp": int(time.time() * 1000),
                    "conversationId": conversation_id,
                    "modelId": model_id,
                })

            # å°è¯•ä½¿ç”¨ Deep Agent
            if use_deep_agent:
                try:
                    result = await self._run_deep_agent(
                        content=content,
                        conversation_id=conversation_id,
                        model_id=model_id,
                        incoming_history=incoming_history,
                        msg_id=msg_id,
                        attachments=attachments,
                    )
                    # å¦‚æœ Deep Agent å®Œæˆæ‰§è¡Œï¼ˆåŒ…æ‹¬è¿”å› {"completed": True}ï¼‰ï¼Œç›´æ¥è¿”å›
                    if result is not None:
                        return result
                    # Deep Agent è¿”å› Noneï¼ˆSDK æœªå®‰è£…ï¼‰ï¼Œé™çº§åˆ° ReAct Agent
                    logger.info("[Agent] Deep Agent ä¸å¯ç”¨ï¼Œé™çº§åˆ° ReAct Agent")
                except Exception as e:
                    logger.warning(
                        f"[Agent] Deep Agent æ‰§è¡Œå¤±è´¥ï¼Œé™çº§åˆ° ReAct Agent: {e}")

            # ä½¿ç”¨ ReAct Agentï¼ˆé»˜è®¤æˆ–é™çº§ï¼‰
            return await self._run_react_agent(
                content=content,
                conversation_id=conversation_id,
                model_id=model_id,
                incoming_history=incoming_history,
                msg_id=msg_id,
            )

        except Exception as e:
            import traceback
            logger.error(f"[Agent] å¤„ç†æ¶ˆæ¯é”™è¯¯: {e}")
            logger.error(f"[Agent] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            if self.send_callback:
                await self.send_callback({
                    "type": "chat_error",
                    "id": msg_id,
                    "timestamp": int(time.time() * 1000),
                    "error": str(e),
                    "conversationId": conversation_id,
                })
            return None

    async def _run_deep_agent(
        self,
        content: str,
        conversation_id: str,
        model_id: Optional[int],
        incoming_history: list,
        msg_id: str,
        attachments: list = [],
    ) -> Optional[dict]:
        """
        ä½¿ç”¨ Deep Agent æ‰§è¡Œ

        Deep Agents æä¾›é«˜çº§èƒ½åŠ›ï¼š
        - ä»»åŠ¡è§„åˆ’ (write_todos)
        - ä¸Šä¸‹æ–‡ç®¡ç† (æ–‡ä»¶ç³»ç»Ÿå·¥å…·)
        - å­æ™ºèƒ½ä½“ç”Ÿæˆ (task å·¥å…·)
        - é•¿æœŸè®°å¿†

        Args:
            content: ç”¨æˆ·è¾“å…¥
            conversation_id: ä¼šè¯ ID
            model_id: æ¨¡å‹ ID
            incoming_history: å†å²æ¶ˆæ¯
            msg_id: æ¶ˆæ¯ ID
            attachments: é™„ä»¶åˆ—è¡¨

        Returns:
            æ‰§è¡Œç»“æœï¼Œå¦‚æœ Deep Agent ä¸å¯ç”¨è¿”å› None
        """
        try:
            from agent import DeepAgentWrapper, MessageSender, create_deep_agent
            from agent.tools import global_tool_registry
            from agent.knowledge_tool import KnowledgeRetrieverTool
            from langchain_core.messages import HumanMessage, AIMessage

            # åˆ›å»ºæ¶ˆæ¯å‘é€å™¨
            async def send_step_callback(step_data):
                if self.send_callback:
                    await self.send_callback(step_data)

            message_sender = MessageSender(send_callback=send_step_callback)

            # è·å–æ‰€æœ‰å·¥å…·
            tools = [
                global_tool_registry.get_tool(name)
                for name in global_tool_registry.list_tools()
                if global_tool_registry.get_tool(name)
            ]

            # è·å–é»˜è®¤çŸ¥è¯†åº“ï¼ˆæå‰è·å–ï¼Œç”¨äºæ³¨å…¥æ£€ç´¢ç»“æœï¼‰
            default_knowledge_id = KnowledgeRetrieverTool.get_default_knowledge()

            # åˆ›å»º Deep Agent
            agent = create_deep_agent(
                model_id=model_id,
                tools=tools,
                message_sender=message_sender
            )

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            if incoming_history:
                for msg in incoming_history:
                    if msg["role"] == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        messages.append(AIMessage(content=msg["content"]))

            # è·å–çŸ¥è¯†åº“å…ƒæ•°æ®ï¼ˆdefault_knowledge_id å·²åœ¨å‰é¢è·å–ï¼‰
            knowledge_metadata = KnowledgeRetrieverTool.get_knowledge_metadata()
            enhanced_content = content

            # å¦‚æœæœ‰é™„ä»¶ï¼Œè®¾ç½®é™„ä»¶è·¯å¾„æ˜ å°„å¹¶æ„å»ºé™„ä»¶ä¸Šä¸‹æ–‡
            attachment_context = ""
            if attachments:
                # è®¾ç½®é™„ä»¶è·¯å¾„æ˜ å°„ï¼Œç”¨äºä¿®æ­£ file_read å·¥å…·çš„è·¯å¾„
                attachment_paths = {}
                attachment_info = []
                for att in attachments:
                    att_name = att.get('name', 'æœªçŸ¥æ–‡ä»¶')
                    att_path = att.get('path', '')
                    att_type = att.get('type', 'other')
                    att_size = att.get('size', 0)
                    # å»ºç«‹æ–‡ä»¶ååˆ°è·¯å¾„çš„æ˜ å°„
                    attachment_paths[att_name] = att_path
                    # æ›´æ¸…æ™°åœ°æ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼Œè®© Agent èƒ½æ­£ç¡®è¯†åˆ«
                    attachment_info.append(f"""æ–‡ä»¶åç§°: {att_name}
æ–‡ä»¶è·¯å¾„: {att_path}
æ–‡ä»¶ç±»å‹: {att_type}
æ–‡ä»¶å¤§å°: {att_size} å­—èŠ‚""")

                # è®¾ç½®é™„ä»¶è·¯å¾„æ˜ å°„ï¼ˆç”¨äºä¿®æ­£ LLM å¯èƒ½ç¼–é€ çš„è·¯å¾„ï¼‰
                DeepAgentWrapper.set_attachment_paths(attachment_paths)

                attachment_context = f"""
[é‡è¦ï¼šç”¨æˆ·ä¸Šä¼ äº†ä»¥ä¸‹æ–‡ä»¶]
{chr(10).join(attachment_info)}

[æŒ‡ä»¤] ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶å¹¶è¯¢é—®ç›¸å…³é—®é¢˜ï¼Œä½ å¿…é¡»å…ˆä½¿ç”¨ file_read å·¥å…·è¯»å–æ–‡ä»¶å†…å®¹ã€‚

è°ƒç”¨ç¤ºä¾‹ï¼š
file_read(file_path="{attachments[0].get('path', '')}")

**ä¸è¦ä½¿ç”¨å…¶ä»–è·¯å¾„ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„æ–‡ä»¶è·¯å¾„ï¼**
"""
                logger.info(
                    f"[DeepAgent] å·²æ„å»ºé™„ä»¶ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(attachment_context)}")
                logger.info(f"[DeepAgent] é™„ä»¶è·¯å¾„æ˜ å°„: {attachment_paths}")

            if default_knowledge_id and knowledge_metadata:
                kb_info = knowledge_metadata.get(default_knowledge_id, {})
                kb_name = kb_info.get("name", "æœªçŸ¥çŸ¥è¯†åº“")
                kb_desc = kb_info.get("description", "")

                # å…ˆè‡ªåŠ¨æ£€ç´¢çŸ¥è¯†åº“ï¼Œè·å–ç›¸å…³å†…å®¹
                knowledge_context = ""
                knowledge_search_result = None
                try:
                    from agent.knowledge_tool import KnowledgeRetrieverTool
                    retriever = KnowledgeRetrieverTool()
                    # æ‰§è¡Œæ£€ç´¢ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼‰
                    knowledge_search_result = await retriever._call_async(
                        query=content,
                        knowledge_id=default_knowledge_id
                    )
                    if knowledge_search_result and "æœªæ‰¾åˆ°" not in knowledge_search_result and "æ²¡æœ‰æ‰¾åˆ°" not in knowledge_search_result and "é”™è¯¯" not in knowledge_search_result:
                        knowledge_context = f"""ä»¥ä¸‹æ˜¯ä»ã€Œ{kb_name}ã€çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š

{knowledge_search_result}"""
                        logger.info(
                            f"[DeepAgent] çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(knowledge_search_result)}")
                    else:
                        logger.info(
                            f"[DeepAgent] çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³å†…å®¹: {knowledge_search_result[:100] if knowledge_search_result else 'empty'}")
                except Exception as e:
                    logger.warning(f"[DeepAgent] çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")

                # ğŸ¯ æ£€æµ‹ç”¨æˆ·æ˜¯å¦æ˜ç¡®è¦æ±‚è”ç½‘æœç´¢
                web_search_keywords = ["è”ç½‘æœç´¢", "ç½‘ä¸Šæœç´¢", "ç½‘ç»œæœç´¢",
                                       "æœç´¢ä¸€ä¸‹", "æœä¸€ä¸‹", "æŸ¥ä¸€ä¸‹", "ç™¾åº¦", "è°·æ­Œ", "æœç´¢çœ‹çœ‹"]
                user_wants_web_search = any(
                    kw in content.lower() for kw in web_search_keywords)

                # ğŸ¯ å…³é”®æ”¹è¿›ï¼šå¦‚æœçŸ¥è¯†åº“æœ‰æ£€ç´¢ç»“æœï¼Œä¸”ç”¨æˆ·æœªæ˜ç¡®è¦æ±‚è”ç½‘æœç´¢ï¼Œç›´æ¥ç”¨ LLM å›ç­”
                if knowledge_context and not attachment_context and not user_wants_web_search:
                    logger.info(f"[DeepAgent] çŸ¥è¯†åº“æœ‰ç»“æœï¼Œç›´æ¥ LLM å›ç­”ï¼ˆè·³è¿‡ Agent å·¥å…·è°ƒç”¨ï¼‰")

                    # æ„å»ºæ¶ˆæ¯
                    llm_messages = []
                    if incoming_history:
                        for msg in incoming_history:
                            if msg["role"] == "user":
                                llm_messages.append(
                                    {"role": "user", "content": msg["content"]})
                            elif msg["role"] == "assistant":
                                llm_messages.append(
                                    {"role": "assistant", "content": msg["content"]})

                    # æ·»åŠ çŸ¥è¯†åº“ä¸Šä¸‹æ–‡å’Œç”¨æˆ·é—®é¢˜
                    user_message = f"""{knowledge_context}

è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š{content}"""
                    llm_messages.append(
                        {"role": "user", "content": user_message})

                    # ç›´æ¥ç”¨ LLM æµå¼å›ç­”
                    full_content = ""
                    chunk_count = 0
                    try:
                        async for chunk in model_router.chat_stream_async(
                            messages=llm_messages,
                            model_id=model_id
                        ):
                            chunk_count += 1
                            full_content += chunk
                            logger.debug(
                                f"[DeepAgent] LLM æµå¼å— #{chunk_count}: {len(chunk)} å­—ç¬¦, ç´¯è®¡: {len(full_content)}")
                            await self.send_callback({
                                "type": "chat_stream_chunk",
                                "id": f"{msg_id}_chunk",
                                "timestamp": int(time.time() * 1000),
                                "conversationId": conversation_id,
                                "content": chunk,
                                "chunkIndex": chunk_count,
                            })

                        # å‘é€ç»“æŸæ¶ˆæ¯
                        await self.send_callback({
                            "type": "chat_stream_end",
                            "id": f"{msg_id}_end",
                            "timestamp": int(time.time() * 1000),
                            "conversationId": conversation_id,
                            "fullContent": full_content,
                        })
                        logger.info(
                            f"[DeepAgent] LLM ç›´æ¥å›ç­”å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(full_content)}")
                        return {"completed": True}
                    except Exception as e:
                        logger.error(
                            f"[DeepAgent] LLM ç›´æ¥å›ç­”å¤±è´¥: {e}ï¼Œé™çº§åˆ° Agent æµç¨‹")
                        # ç»§ç»­èµ° Agent æµç¨‹ä½œä¸ºé™çº§

                # ä½¿ç”¨ç³»ç»Ÿæç¤ºæ ¼å¼ï¼Œé¿å…è¾“å‡ºç»™ç”¨æˆ·
                if attachment_context:
                    # æœ‰é™„ä»¶æ—¶ï¼Œä¼˜å…ˆå¤„ç†é™„ä»¶
                    enhanced_content = f"""{attachment_context}

[ç”¨æˆ·é—®é¢˜]
{content}

[æ³¨æ„] ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ file_read å·¥å…·è¯»å–å¹¶åˆ†ææ–‡ä»¶å†…å®¹ã€‚"""
                elif knowledge_context:
                    # æœ‰çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
                    if user_wants_web_search:
                        # ç”¨æˆ·æ˜ç¡®è¦æ±‚è”ç½‘æœç´¢ï¼ŒAgent éœ€è¦è°ƒç”¨ web_search
                        enhanced_content = f"""{knowledge_context}

[ç”¨æˆ·é—®é¢˜]
{content}

[ç”¨æˆ·è¦æ±‚] ç”¨æˆ·æ˜ç¡®è¦æ±‚è”ç½‘æœç´¢ï¼Œè¯·è°ƒç”¨ web_search å·¥å…·æœç´¢ç½‘ç»œä¿¡æ¯ï¼Œå¹¶ç»“åˆçŸ¥è¯†åº“å†…å®¹å›ç­”ã€‚"""
                        logger.info(f"[DeepAgent] ç”¨æˆ·è¦æ±‚è”ç½‘æœç´¢ï¼Œèµ° Agent æµç¨‹")
                    else:
                        enhanced_content = f"""{knowledge_context}

[ç”¨æˆ·é—®é¢˜]
{content}"""
                else:
                    # çŸ¥è¯†åº“æ£€ç´¢æ— ç»“æœï¼ŒAgent å¯ä»¥ä½¿ç”¨å…¶ä»–å·¥å…·è¡¥å……
                    enhanced_content = f"""[Context]
ç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“: {kb_name}
çŸ¥è¯†åº“ ID: {default_knowledge_id}
æè¿°: {kb_desc or 'æ— '}

[é‡è¦] çŸ¥è¯†åº“è‡ªåŠ¨æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚ä½ å¯ä»¥ï¼š
1. ä½¿ç”¨ web_search å·¥å…·æœç´¢ç½‘ç»œä¿¡æ¯
2. æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ï¼Œå¹¶å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æš‚æ— ç›¸å…³å†…å®¹

[User Question]
{content}"""
            elif attachment_context:
                # åªæœ‰é™„ä»¶ï¼Œæ²¡æœ‰çŸ¥è¯†åº“
                enhanced_content = f"""{attachment_context}

[ç”¨æˆ·é—®é¢˜]
{content}

[å†æ¬¡æé†’] ä½ å¿…é¡»è°ƒç”¨ file_read å·¥å…·æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼"""
                logger.info(
                    f"[DeepAgent] ä½¿ç”¨é™„ä»¶ä¸Šä¸‹æ–‡ï¼ˆæ— çŸ¥è¯†åº“ï¼‰ï¼Œenhanced_content é•¿åº¦: {len(enhanced_content)}")

            logger.info(
                f"[DeepAgent] å¼€å§‹æ‰§è¡Œï¼Œå·¥å…·æ•°é‡: {len(tools)}, å·²é€‰çŸ¥è¯†åº“: {default_knowledge_id}")

            # æµå¼æ‰§è¡Œ
            # æ³¨æ„ï¼šDeep Agent å†…éƒ¨é€šè¿‡ message_sender å‘é€ agent_step æ¶ˆæ¯
            # è¿™é‡Œåªéœ€è¦å¤„ç†æµå¼å†…å®¹å’Œç»“æŸæ¶ˆæ¯
            full_content = ""
            has_final_answer = False

            async for step in agent.astream(
                input_text=enhanced_content,
                messages=messages if messages else None,
                conversation_id=conversation_id
            ):
                step_type = step.get("step_type", "")
                step_content = step.get("content", "")

                # å¦‚æœæ˜¯æµå¼å†…å®¹ï¼Œå‘é€æµå¼æ¶ˆæ¯
                if step_type == "stream_chunk":
                    full_content = step.get(
                        "full_content", full_content + step_content)
                    await self.send_callback({
                        "type": "chat_stream_chunk",
                        "id": f"{msg_id}_chunk",
                        "timestamp": int(time.time() * 1000),
                        "conversationId": conversation_id,
                        "content": step_content,
                        "chunkIndex": step.get("iteration", 0),
                    })

                # å¦‚æœæ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œå‘é€ç»“æŸæ¶ˆæ¯
                if step_type == "answer":
                    has_final_answer = True
                    full_content = step_content if step_content else full_content
                    await self.send_callback({
                        "type": "chat_stream_end",
                        "id": f"{msg_id}_end",
                        "timestamp": int(time.time() * 1000),
                        "conversationId": conversation_id,
                        "fullContent": full_content,
                    })

                # æ”¶é›†æœ€ç»ˆå†…å®¹ï¼ˆç”¨äºåç»­å‘é€ç»“æŸæ¶ˆæ¯ï¼‰
                if step_content and step_type in ["thought", "answer"]:
                    full_content = step_content

            # å¦‚æœæµå¼å¾ªç¯ç»“æŸä½†æ²¡æœ‰æ”¶åˆ° answer ç±»å‹ï¼Œæ‰‹åŠ¨å‘é€ç»“æŸæ¶ˆæ¯
            if not has_final_answer:
                logger.info(
                    f"[DeepAgent] æµå¼ç»“æŸï¼Œå‘é€ç»“æŸæ¶ˆæ¯ï¼Œå†…å®¹é•¿åº¦: {len(full_content)}")
                await self.send_callback({
                    "type": "chat_stream_end",
                    "id": f"{msg_id}_end",
                    "timestamp": int(time.time() * 1000),
                    "conversationId": conversation_id,
                    "fullContent": full_content,
                })

            # å¦‚æœæœ‰é™„ä»¶ï¼Œå‘é€ knowledge_ask_add æ¶ˆæ¯è¯¢é—®ç”¨æˆ·æ˜¯å¦æ·»åŠ åˆ°çŸ¥è¯†åº“
            # æ³¨æ„ï¼šå»¶è¿Ÿå‘é€ï¼Œç¡®ä¿å‰ç«¯æœ‰è¶³å¤Ÿæ—¶é—´å¤„ç† chat_stream_end å¹¶æ›´æ–° UI
            if attachments:
                import uuid
                # å»¶è¿Ÿ 500ms å‘é€ï¼Œç¡®ä¿å‰ç«¯å·²å®Œæˆæ¶ˆæ¯æ¸²æŸ“
                await asyncio.sleep(0.5)
                for att in attachments:
                    await self.send_callback({
                        "type": "knowledge_ask_add",
                        "id": f"{msg_id}_ask_{uuid.uuid4().hex[:8]}",
                        "timestamp": int(time.time() * 1000),
                        "conversationId": conversation_id,
                        "content": f"å·²åˆ†ææ–‡ä»¶ã€Œ{att.get('name', 'æœªçŸ¥æ–‡ä»¶')}ã€ï¼Œæ˜¯å¦éœ€è¦å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼Ÿ",
                        "attachment": {
                            "id": str(uuid.uuid4()),
                            "name": att.get("name", "æœªçŸ¥æ–‡ä»¶"),
                            "path": att.get("path", ""),
                            "size": att.get("size", 0),
                            "type": att.get("type", "file"),
                            "mimeType": att.get("mimeType", ""),
                        },
                    })
                    logger.info(
                        f"[DeepAgent] å‘é€ knowledge_ask_add æ¶ˆæ¯ï¼Œé™„ä»¶: {att.get('name')}")

            logger.info(f"[DeepAgent] æ‰§è¡Œå®Œæˆ")
            return {"completed": True}  # è¿”å›æ ‡è®°è¡¨ç¤º Deep Agent å·²å®Œæˆæ‰§è¡Œ

        except ImportError as e:
            logger.info(f"[DeepAgent] SDK æœªå®‰è£…ï¼Œé™çº§åˆ° ReAct Agent: {e}")
            return None  # Deep Agent ä¸å¯ç”¨ï¼Œè¿”å› None è®©è°ƒç”¨è€…é™çº§
        except Exception as e:
            logger.error(f"[DeepAgent] æ‰§è¡Œé”™è¯¯: {e}")
            raise  # æŠ›å‡ºå¼‚å¸¸è®©è°ƒç”¨è€…å¤„ç†

    async def _run_react_agent(
        self,
        content: str,
        conversation_id: str,
        model_id: Optional[int],
        incoming_history: list,
        msg_id: str,
    ) -> Optional[dict]:
        """
        ä½¿ç”¨ ReAct Agent æ‰§è¡Œ

        ReAct (Reasoning + Acting) å¾ªç¯ï¼š
        1. æ€è€ƒï¼šåˆ†æé—®é¢˜
        2. è¡ŒåŠ¨ï¼šè°ƒç”¨å·¥å…·
        3. è§‚å¯Ÿï¼šæŸ¥çœ‹ç»“æœ

        Args:
            content: ç”¨æˆ·è¾“å…¥
            conversation_id: ä¼šè¯ ID
            model_id: æ¨¡å‹ ID
            incoming_history: å†å²æ¶ˆæ¯
            msg_id: æ¶ˆæ¯ ID

        Returns:
            æ‰§è¡Œç»“æœ
        """
        from agent import ReActAgent
        from agent.knowledge_tool import KnowledgeRetrieverTool
        from langchain_core.messages import HumanMessage

        # å¦‚æœç”¨æˆ·é€‰æ‹©äº†çŸ¥è¯†åº“ï¼Œåœ¨æ¶ˆæ¯å‰é¢æ³¨å…¥ä¸Šä¸‹æ–‡
        default_knowledge_id = KnowledgeRetrieverTool.get_default_knowledge()
        knowledge_metadata = KnowledgeRetrieverTool.get_knowledge_metadata()
        enhanced_content = content

        if default_knowledge_id and knowledge_metadata:
            kb_info = knowledge_metadata.get(default_knowledge_id, {})
            kb_name = kb_info.get("name", "æœªçŸ¥çŸ¥è¯†åº“")
            kb_desc = kb_info.get("description", "")

            # å…ˆè‡ªåŠ¨æ£€ç´¢çŸ¥è¯†åº“ï¼Œè·å–ç›¸å…³å†…å®¹
            knowledge_context = ""
            try:
                from agent.knowledge_tool import KnowledgeRetrieverTool
                retriever = KnowledgeRetrieverTool()
                # æ‰§è¡Œæ£€ç´¢ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼‰
                search_result = await retriever._call_async(
                    query=content,
                    knowledge_id=default_knowledge_id
                )
                if search_result and "æœªæ‰¾åˆ°" not in search_result and "æ²¡æœ‰æ‰¾åˆ°" not in search_result and "é”™è¯¯" not in search_result:
                    knowledge_context = f"""
ã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘
ä»¥ä¸‹æ˜¯ä»ã€Œ{kb_name}ã€çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š

{search_result}

ã€å›ç­”è¦æ±‚ã€‘
1. **ä¼˜å…ˆ**åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“æ£€ç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹å·²è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œç›´æ¥å›ç­”å³å¯
3. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸å®Œæ•´æˆ–æ— æ³•å®Œå…¨å›ç­”ï¼Œå¯ä»¥è°ƒç”¨ web_search ç­‰å·¥å…·è¡¥å……ä¿¡æ¯
4. å›ç­”æ—¶æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆçŸ¥è¯†åº“/ç½‘ç»œæœç´¢/è‡ªèº«çŸ¥è¯†ï¼‰"""
                    logger.info(
                        f"[ReActAgent] çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸï¼Œç»“æœé•¿åº¦: {len(search_result)}")
                else:
                    logger.info(f"[ReActAgent] çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            except Exception as e:
                logger.warning(f"[ReActAgent] çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")

            # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ·»åŠ ä¸Šä¸‹æ–‡æç¤º
            if knowledge_context:
                enhanced_content = f"""{knowledge_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘{content}

ã€æŒ‡ä»¤ã€‘è¯·åŸºäºçŸ¥è¯†åº“æ£€ç´¢ç»“æœå›ç­”é—®é¢˜ã€‚å¦‚æœæ£€ç´¢ç»“æœä¸å®Œæ•´ï¼Œå¯ä»¥è¡¥å……ä½ çš„çŸ¥è¯†ã€‚"""
            else:
                enhanced_content = f"""ã€é‡è¦ã€‘ç”¨æˆ·å·²æ˜ç¡®é€‰æ‹©çŸ¥è¯†åº“ï¼š{kb_name}
çŸ¥è¯†åº“ID: {default_knowledge_id}
çŸ¥è¯†åº“æè¿°: {kb_desc or 'æ— '}

ã€ç”¨æˆ·é—®é¢˜ã€‘{content}

ã€æŒ‡ä»¤ã€‘çŸ¥è¯†åº“è‡ªåŠ¨æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚ä½ å¯ä»¥ï¼š
1. ä½¿ç”¨ web_search å·¥å…·æœç´¢ç½‘ç»œä¿¡æ¯
2. æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ï¼Œå¹¶å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æš‚æ— ç›¸å…³å†…å®¹"""

        # åˆ›å»º Agent å®ä¾‹
        agent = ReActAgent(model_id=model_id)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if incoming_history:
            for msg in incoming_history:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                # å…¶ä»–è§’è‰²çš„æ¶ˆæ¯å¯ä»¥åç»­æ·»åŠ 

        logger.info(f"[ReActAgent] å¼€å§‹æ‰§è¡Œï¼Œå·²é€‰çŸ¥è¯†åº“: {default_knowledge_id}")

        # æ‰§è¡Œ Agentï¼ˆæµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼‰- ä½¿ç”¨å¢å¼ºåçš„å†…å®¹
        sent_step_count = 0  # å·²å‘é€çš„æ­¥éª¤æ•°é‡
        async for event in agent.astream(
            input_text=enhanced_content,
            messages=messages if messages else None,
            conversation_id=conversation_id
        ):
            node_name = event.get("node", "")
            state_update = event.get("update", {})

            logger.info(
                f"[Agent] äº‹ä»¶: node={node_name}, update keys={list(state_update.keys())}")

            # å‘é€ Agent æ­¥éª¤æ¶ˆæ¯
            if self.send_callback:
                # å¤„ç†æ­¥éª¤æ›´æ–° - åªå‘é€æ–°å¢çš„æ­¥éª¤
                if "steps" in state_update:
                    all_steps = state_update["steps"]
                    new_steps = all_steps[sent_step_count:]  # åªå–æ–°å¢çš„æ­¥éª¤
                    sent_step_count = len(all_steps)  # æ›´æ–°å·²å‘é€æ•°é‡

                    for step in new_steps:
                        step_data = {
                            "type": "agent_step",
                            "id": f"{msg_id}_step_{step.get('type', 'unknown')}_{sent_step_count}",
                            "timestamp": int(time.time() * 1000),
                            "conversationId": conversation_id,
                            "stepType": step.get("type", "thought"),
                            "content": step.get("content", ""),
                            "toolCall": step.get("tool_call"),
                            "iteration": state_update.get("iteration_count", 0),
                        }
                        logger.info(
                            f"[Agent] å‘é€æ­¥éª¤: {step_data['stepType']}, content={step_data['content'][:50] if step_data['content'] else 'empty'}")
                        await self.send_callback(step_data)

                # å¦‚æœæœ‰æœ€ç»ˆè¾“å‡ºï¼Œå‘é€ç»“æŸæ¶ˆæ¯ï¼ˆæµå¼è¾“å‡ºï¼‰
                if state_update.get("output") and state_update.get("should_finish"):
                    final_content = state_update["output"]

                    # å‘é€æµå¼å†…å®¹å—ï¼ˆæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼‰
                    chunk_size = 20  # æ¯å—å­—ç¬¦æ•°
                    for i in range(0, len(final_content), chunk_size):
                        chunk = final_content[i:i + chunk_size]
                        await self.send_callback({
                            "type": "chat_stream_chunk",
                            "id": f"{msg_id}_chunk_{i}",
                            "timestamp": int(time.time() * 1000),
                            "conversationId": conversation_id,
                            "content": chunk,
                            "chunkIndex": i // chunk_size,
                        })
                        # å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼æ•ˆæœ
                        await asyncio.sleep(0.01)

                    # å‘é€ç»“æŸæ¶ˆæ¯
                    await self.send_callback({
                        "type": "chat_stream_end",
                        "id": f"{msg_id}_end",
                        "timestamp": int(time.time() * 1000),
                        "conversationId": conversation_id,
                        "fullContent": final_content,
                    })

        return None  # é€šè¿‡æµå¼æ¶ˆæ¯å‘é€ï¼Œä¸è¿”å›å“åº”

    async def _handle_system_status(self, message: dict) -> dict:
        """å¤„ç†ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢"""
        return {
            "type": "system_status",
            "id": message.get("id"),
            "timestamp": int(time.time() * 1000),
            "status": "ready",
            "conversations": len(self.conversations),
            "registered_models": len(model_router._models)
        }

    async def _handle_model_register(self, message: dict) -> dict:
        """å¤„ç†æ¨¡å‹æ³¨å†Œè¯·æ±‚"""
        try:
            model_id = message.get("modelId")
            config_data = message.get("config", {})

            if not model_id:
                return self._error_response("ç¼ºå°‘ modelId", message.get("id"))

            # æ„å»ºæ¨¡å‹é…ç½®
            provider_str = config_data.get("provider", "openai")
            try:
                provider = ModelProvider(provider_str)
            except ValueError:
                return self._error_response(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider_str}", message.get("id"))

            config = ModelConfig(
                provider=provider,
                model_id=config_data.get("modelId", ""),
                api_key=config_data.get("apiKey"),
                api_base_url=config_data.get("apiBaseUrl"),
                host=config_data.get("host"),
                max_tokens=config_data.get("maxTokens", 4096),
                temperature=config_data.get("temperature", 0.7),
                extra_params=config_data.get("extraParams"),
            )

            # æ³¨å†Œæ¨¡å‹
            model_router.register_model(model_id, config)

            return {
                "type": "model_register_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "modelId": model_id,
                "provider": provider.value,
                "model": config.model_id
            }
        except Exception as e:
            logger.error(f"æ³¨å†Œæ¨¡å‹å¤±è´¥: {e}")
            return self._error_response(str(e), message.get("id"))

    async def _handle_model_unregister(self, message: dict) -> dict:
        """å¤„ç†æ¨¡å‹æ³¨é”€è¯·æ±‚"""
        model_id = message.get("modelId")

        if not model_id:
            return self._error_response("ç¼ºå°‘ modelId", message.get("id"))

        model_router.unregister_model(model_id)

        return {
            "type": "model_unregister_response",
            "id": message.get("id"),
            "timestamp": int(time.time() * 1000),
            "success": True,
            "modelId": model_id
        }

    async def _handle_model_test(self, message: dict) -> dict:
        """å¤„ç†æ¨¡å‹è¿æ¥æµ‹è¯•è¯·æ±‚"""
        model_id = message.get("modelId")

        if not model_id:
            return self._error_response("ç¼ºå°‘ modelId", message.get("id"))

        result = await model_router.test_connection(model_id)

        return {
            "type": "model_test_response",
            "id": message.get("id"),
            "timestamp": int(time.time() * 1000),
            **result
        }

    async def _handle_model_config_sync(self, message: dict) -> dict:
        """å¤„ç†æ¨¡å‹é…ç½®åŒæ­¥è¯·æ±‚"""
        try:
            configs = message.get("configs", [])
            registered_count = 0
            errors = []

            for config_data in configs:
                try:
                    model_id = config_data.get("id")
                    if not model_id:
                        continue

                    # æ„å»ºæ¨¡å‹é…ç½®
                    provider_str = config_data.get("provider", "openai")
                    try:
                        provider = ModelProvider(provider_str)
                    except ValueError:
                        errors.append(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider_str}")
                        continue

                    config = ModelConfig(
                        id=model_id,
                        provider=provider,
                        model_id=config_data.get("modelId", ""),
                        api_key=config_data.get("apiKey"),
                        api_base_url=config_data.get("apiBaseUrl"),
                        host=config_data.get("host"),
                        max_tokens=config_data.get("maxTokens", 4096),
                        temperature=config_data.get("temperature", 0.7),
                    )

                    # æ³¨å†Œæ¨¡å‹
                    model_router.register_model(model_id, config)
                    registered_count += 1
                    logger.info(
                        f"å·²æ³¨å†Œæ¨¡å‹: {config.model_id} (provider: {provider})")

                except Exception as e:
                    errors.append(f"æ³¨å†Œæ¨¡å‹ {config_data.get('id')} å¤±è´¥: {str(e)}")
                    logger.error(f"æ³¨å†Œæ¨¡å‹å¤±è´¥: {e}")

            return {
                "type": "model_config_sync_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "registered_count": registered_count,
                "errors": errors if errors else None,
            }

        except Exception as e:
            logger.error(f"å¤„ç†æ¨¡å‹é…ç½®åŒæ­¥é”™è¯¯: {e}")
            return self._error_response(str(e), message.get("id"))

    async def _generate_mock_response(self, content: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”ï¼ˆåç»­æ›¿æ¢ä¸ºå®é™… AI å¤„ç†ï¼‰"""
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await self._async_sleep(0.1)

        # ç®€å•çš„å“åº”é€»è¾‘
        responses = {
            "ä½ å¥½": "ä½ å¥½ï¼æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚",
            "hello": "Hello! I'm your AI assistant.",
            "å¸®åŠ©": "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š\n1. å›ç­”é—®é¢˜\n2. å¤„ç†æ–‡æœ¬\n3. æä¾›å»ºè®®\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ",
            "help": "I can help you with:\n1. Answering questions\n2. Processing text\n3. Providing suggestions\n\nWhat do you need?",
        }

        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¾å“åº”
        content_lower = content.lower().strip()
        for key, response in responses.items():
            if key.lower() in content_lower:
                return response

        # é»˜è®¤å“åº”
        return f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š\"{content}\"\n\næˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œç›®å‰å¤„äºæµ‹è¯•æ¨¡å¼ã€‚åç»­å°†æ¥å…¥å®é™…çš„æ™ºèƒ½ä½“è¿›è¡Œå¤„ç†ã€‚"

    async def _async_sleep(self, seconds: float):
        """å¼‚æ­¥ç¡çœ """
        import asyncio
        await asyncio.sleep(seconds)

    def _error_response(self, error: str, msg_id: Optional[str] = None) -> dict:
        """ç”Ÿæˆé”™è¯¯å“åº”"""
        return {
            "type": "chat_error",
            "id": msg_id,
            "timestamp": int(time.time() * 1000),
            "error": error,
            "code": "PROCESSING_ERROR"
        }

    # ===== Ollama ç›¸å…³å¤„ç†å™¨ =====

    async def _handle_ollama_status(self, message: dict) -> dict:
        """å¤„ç† Ollama çŠ¶æ€æŸ¥è¯¢"""
        host = message.get("host", "http://127.0.0.1:11434")

        try:
            status = await check_ollama_status(host)
            return {
                "type": "ollama_status_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                **status.to_dict()
            }
        except Exception as e:
            logger.error(f"æ£€æŸ¥ Ollama çŠ¶æ€å¤±è´¥: {e}")
            return {
                "type": "ollama_status_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "running": False,
                "host": host,
                "error": str(e),
            }

    async def _handle_ollama_models(self, message: dict) -> dict:
        """å¤„ç† Ollama æ¨¡å‹åˆ—è¡¨æŸ¥è¯¢"""
        host = message.get("host", "http://127.0.0.1:11434")

        try:
            models = await get_ollama_models(host)
            return {
                "type": "ollama_models_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "host": host,
                "models": [m.to_dict() for m in models],
                "count": len(models),
            }
        except Exception as e:
            logger.error(f"è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "type": "ollama_models_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "host": host,
                "error": str(e),
                "models": [],
                "count": 0,
            }

    async def _handle_ollama_test(self, message: dict) -> dict:
        """å¤„ç† Ollama è¿æ¥æµ‹è¯•"""
        host = message.get("host", "http://127.0.0.1:11434")

        try:
            client = get_ollama_client(host)
            result = await client.test_connection()
            return {
                "type": "ollama_test_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                **result
            }
        except Exception as e:
            logger.error(f"æµ‹è¯• Ollama è¿æ¥å¤±è´¥: {e}")
            return {
                "type": "ollama_test_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "host": host,
                "error": str(e),
            }

    # ==================== Skills æŠ€èƒ½ç›¸å…³å¤„ç† ====================

    async def _handle_skill_list(self, message: dict) -> dict:
        """
        å¤„ç†æŠ€èƒ½åˆ—è¡¨æŸ¥è¯¢

        è¿”å›æ‰€æœ‰å·²æ³¨å†Œçš„æŠ€èƒ½ä¿¡æ¯ã€‚
        """
        try:
            from agent.skills import global_skill_registry

            skills = global_skill_registry.list_skills(enabled_only=False)
            skill_list = []

            for skill in skills:
                skill_list.append({
                    "name": skill.name,
                    "displayName": skill.config.metadata.display_name,
                    "description": skill.description,
                    "type": skill.config.type.value,
                    "trigger": skill.config.trigger.value,
                    "enabled": skill.enabled,
                    "tags": skill.config.metadata.tags,
                    "icon": skill.config.metadata.icon,
                    "version": skill.config.metadata.version,
                    "author": skill.config.metadata.author,
                })

            return {
                "type": "skill_list_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "skills": skill_list,
                "count": len(skill_list),
            }

        except Exception as e:
            logger.error(f"è·å–æŠ€èƒ½åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "type": "skill_list_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
                "skills": [],
                "count": 0,
            }

    async def _handle_skill_execute(self, message: dict) -> dict:
        """
        å¤„ç†æŠ€èƒ½æ‰§è¡Œè¯·æ±‚

        æ‰§è¡ŒæŒ‡å®šçš„æŠ€èƒ½å¹¶è¿”å›ç»“æœã€‚
        """
        skill_name = message.get("skillName")
        parameters = message.get("parameters", {})

        if not skill_name:
            return {
                "type": "skill_execute_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘æŠ€èƒ½åç§°",
            }

        try:
            from agent.skills import global_skill_registry

            # æ‰§è¡ŒæŠ€èƒ½
            result = await global_skill_registry.execute_skill(skill_name, **parameters)

            return {
                "type": "skill_execute_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "skillName": skill_name,
                "result": result,
            }

        except ValueError as e:
            logger.warning(f"æŠ€èƒ½æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "type": "skill_execute_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "skillName": skill_name,
                "error": str(e),
            }
        except Exception as e:
            logger.error(f"æŠ€èƒ½æ‰§è¡Œé”™è¯¯: {e}")
            return {
                "type": "skill_execute_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "skillName": skill_name,
                "error": str(e),
            }

    async def _handle_skill_reload(self, message: dict) -> dict:
        """
        å¤„ç†æŠ€èƒ½é‡è½½è¯·æ±‚

        é‡æ–°åŠ è½½æŒ‡å®šçš„æŠ€èƒ½æˆ–æ‰€æœ‰æŠ€èƒ½ã€‚
        """
        skill_name = message.get("skillName")  # å¦‚æœä¸ºç©ºï¼Œé‡è½½æ‰€æœ‰æŠ€èƒ½

        try:
            from agent.skills import global_skill_registry, SkillLoader
            from agent.tools import global_tool_registry

            if skill_name:
                # é‡è½½æŒ‡å®šæŠ€èƒ½
                loader = SkillLoader(
                    tool_registry=global_tool_registry,
                    skill_registry=global_skill_registry
                )
                skill = loader.reload_skill(skill_name)

                if skill:
                    return {
                        "type": "skill_reload_response",
                        "id": message.get("id"),
                        "timestamp": int(time.time() * 1000),
                        "success": True,
                        "skillName": skill_name,
                        "message": f"æŠ€èƒ½ {skill_name} é‡è½½æˆåŠŸ",
                    }
                else:
                    return {
                        "type": "skill_reload_response",
                        "id": message.get("id"),
                        "timestamp": int(time.time() * 1000),
                        "success": False,
                        "skillName": skill_name,
                        "error": f"æŠ€èƒ½ {skill_name} é‡è½½å¤±è´¥",
                    }
            else:
                # é‡è½½æ‰€æœ‰æŠ€èƒ½
                import os
                skills_dir = os.path.join(os.path.dirname(__file__), "skills")

                if not os.path.exists(skills_dir):
                    return {
                        "type": "skill_reload_response",
                        "id": message.get("id"),
                        "timestamp": int(time.time() * 1000),
                        "success": True,
                        "message": "æŠ€èƒ½ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€é‡è½½",
                    }

                # æ¸…ç©ºç°æœ‰è‡ªå®šä¹‰æŠ€èƒ½
                global_skill_registry.clear()

                # é‡æ–°æ³¨å†Œå†…ç½®æŠ€èƒ½
                from agent.skills import init_builtin_skills
                init_builtin_skills(global_skill_registry)

                # é‡æ–°åŠ è½½è‡ªå®šä¹‰æŠ€èƒ½
                loader = SkillLoader(
                    tool_registry=global_tool_registry,
                    skill_registry=global_skill_registry
                )
                skills = loader.load_from_directory(skills_dir)

                return {
                    "type": "skill_reload_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": True,
                    "message": f"é‡è½½å®Œæˆï¼Œå…± {len(global_skill_registry)} ä¸ªæŠ€èƒ½",
                    "count": len(global_skill_registry),
                }

        except Exception as e:
            logger.error(f"é‡è½½æŠ€èƒ½å¤±è´¥: {e}")
            return {
                "type": "skill_reload_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    # ==================== Knowledge çŸ¥è¯†åº“ç›¸å…³æ¶ˆæ¯å¤„ç† ====================
    # æ³¨æ„ï¼šçŸ¥è¯†åº“çš„åˆ›å»º/åˆ é™¤/åˆ—è¡¨ç°åœ¨ç»Ÿä¸€ç”±å‰ç«¯é€šè¿‡ FrontendBridge å¤„ç†
    # Python ç«¯åªä¿ç•™ LanceDB å‘é‡æ“ä½œå’Œæœç´¢åŠŸèƒ½

    async def _handle_knowledge_create(self, message: dict) -> dict:
        """
        å¤„ç†åˆ›å»º LanceDB é›†åˆè¯·æ±‚ï¼ˆä»…å‘é‡å­˜å‚¨ï¼‰

        æ­¤æ¥å£ä»…ä¾›å‰ç«¯è°ƒç”¨ï¼Œç”¨äºåˆ›å»º LanceDB å‘é‡å­˜å‚¨é›†åˆã€‚
        å®Œæ•´çš„çŸ¥è¯†åº“åˆ›å»ºæµç¨‹ï¼ˆSQLite + LanceDBï¼‰ç”±å‰ç«¯ç»Ÿä¸€å¤„ç†ã€‚
        """
        knowledge_id = message.get("knowledgeId")
        embedding_model = message.get("embeddingModel", "ollama")
        embedding_model_name = message.get(
            "embeddingModelName", "nomic-embed-text")

        if not knowledge_id:
            return {
                "type": "knowledge_create_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "çŸ¥è¯†åº“ ID ä¸èƒ½ä¸ºç©º",
            }

        try:
            from rag.vectorstore import get_vectorstore
            from rag.embeddings import EmbeddingService, EmbeddingModelType

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = get_vectorstore()

            # ç¡®å®šåµŒå…¥æ¨¡å‹ç±»å‹å’Œç»´åº¦
            model_type = EmbeddingModelType.OLLAMA if embedding_model == "ollama" else EmbeddingModelType.OPENAI
            embedding_service = EmbeddingService(
                model_type=model_type,
                model_name=embedding_model_name,
            )

            # åˆ›å»ºé›†åˆ
            success = vectorstore.create_collection(
                knowledge_id, embedding_service.dimension)

            return {
                "type": "knowledge_create_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": success,
                "knowledgeId": knowledge_id,
            }

        except Exception as e:
            logger.error(f"åˆ›å»º LanceDB é›†åˆé”™è¯¯: {e}")
            return {
                "type": "knowledge_create_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    async def _handle_knowledge_delete(self, message: dict) -> dict:
        """
        å¤„ç†åˆ é™¤ LanceDB é›†åˆè¯·æ±‚ï¼ˆä»…å‘é‡å­˜å‚¨ï¼‰

        æ­¤æ¥å£ä»…ä¾›å‰ç«¯è°ƒç”¨ï¼Œç”¨äºåˆ é™¤ LanceDB å‘é‡å­˜å‚¨é›†åˆã€‚
        å®Œæ•´çš„çŸ¥è¯†åº“åˆ é™¤æµç¨‹ï¼ˆSQLite + LanceDBï¼‰ç”±å‰ç«¯ç»Ÿä¸€å¤„ç†ã€‚
        """
        knowledge_id = message.get("knowledgeId")

        if not knowledge_id:
            return {
                "type": "knowledge_delete_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "çŸ¥è¯†åº“ ID ä¸èƒ½ä¸ºç©º",
            }

        try:
            from rag.vectorstore import get_vectorstore

            vectorstore = get_vectorstore()
            success = vectorstore.delete_collection(knowledge_id)

            return {
                "type": "knowledge_delete_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": success,
                "knowledgeId": knowledge_id,
            }

        except Exception as e:
            logger.error(f"åˆ é™¤ LanceDB é›†åˆé”™è¯¯: {e}")
            return {
                "type": "knowledge_delete_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "knowledgeId": knowledge_id,
                "error": str(e),
            }

    async def _handle_knowledge_list(self, message: dict) -> dict:
        """
        å¤„ç†è·å– LanceDB é›†åˆåˆ—è¡¨è¯·æ±‚

        æ³¨æ„ï¼šæ­¤æ¥å£è¿”å›çš„æ˜¯ LanceDB ä¸­çš„é›†åˆä¿¡æ¯ï¼Œä¸åŒ…å« SQLite ä¸­çš„å…ƒæ•°æ®ã€‚
        å®Œæ•´çš„çŸ¥è¯†åº“åˆ—è¡¨è¯·é€šè¿‡ FrontendBridge è°ƒç”¨ knowledgeService.listKnowledge()
        """
        try:
            from rag.vectorstore import get_vectorstore

            vectorstore = get_vectorstore()
            stats = vectorstore.get_stats()

            # å°†é›†åˆä¿¡æ¯è½¬æ¢ä¸ºçŸ¥è¯†åº“æ ¼å¼
            knowledge_list = []
            for collection in stats.get("collections", []):
                knowledge_list.append({
                    "id": collection["id"],
                    "name": collection["id"],  # ä½¿ç”¨é›†åˆ ID ä½œä¸ºåç§°
                    "documentCount": collection["document_count"],
                    "totalChunks": 0,
                    "embeddingModel": "ollama",
                    "embeddingModelName": "nomic-embed-text",
                    "createdAt": int(time.time() * 1000),
                    "updatedAt": int(time.time() * 1000),
                })

            return {
                "type": "knowledge_list_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "knowledge": knowledge_list,
                "count": len(knowledge_list),
                "note": "æ­¤ä¸º LanceDB é›†åˆåˆ—è¡¨ï¼Œå®Œæ•´çŸ¥è¯†åº“åˆ—è¡¨è¯·é€šè¿‡ FrontendBridge è·å–",
            }

        except Exception as e:
            logger.error(f"è·å– LanceDB é›†åˆåˆ—è¡¨é”™è¯¯: {e}")
            return {
                "type": "knowledge_list_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "knowledge": [],
                "count": 0,
                "error": str(e),
            }

    async def _handle_knowledge_add_document(self, message: dict) -> dict:
        """å¤„ç†æ·»åŠ æ–‡æ¡£è¯·æ±‚"""
        knowledge_id = message.get("knowledgeId")
        file_path = message.get("filePath")
        original_file_name = message.get("originalFileName")  # åŸå§‹æ–‡ä»¶å

        if not knowledge_id or not file_path:
            return {
                "type": "knowledge_add_document_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "çŸ¥è¯†åº“ ID å’Œæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º",
            }

        try:
            from rag.vectorstore import get_vectorstore, Document
            from rag.document_processor import DocumentProcessor
            from rag.text_splitter import SmartTextSplitter
            from rag.embeddings import get_embedding_service
            import uuid
            import os

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return {
                    "type": "knowledge_add_document_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": False,
                    "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                }

            # è·å–æ–‡ä»¶æ‰©å±•å
            file_ext = os.path.splitext(file_path)[1].lower()
            image_extensions = ['.jpg', '.jpeg',
                                '.png', '.gif', '.webp', '.bmp']
            ocr_text = None
            ocr_blocks = None  # OCR è¾¹ç•Œæ¡†ä¿¡æ¯

            # å¤„ç†å›¾ç‰‡æ–‡ä»¶ï¼šæ‰§è¡Œ OCR è¯†åˆ«
            if file_ext in image_extensions:
                try:
                    from ocr_service import ocr_recognize_image
                    import json
                    logger.info(f"[Knowledge] å¯¹å›¾ç‰‡æ‰§è¡Œ OCR: {file_path}")
                    ocr_result = ocr_recognize_image(file_path)
                    if ocr_result and ocr_result.get('success'):
                        ocr_text = ocr_result.get('text', '')
                        # ä¿å­˜å®Œæ•´çš„ blocks ä¿¡æ¯ï¼ˆåŒ…å«è¾¹ç•Œæ¡†å’Œè¯†åˆ«ç‡ï¼‰
                        if ocr_result.get('blocks'):
                            ocr_blocks = json.dumps(
                                ocr_result.get('blocks'), ensure_ascii=False)
                        logger.info(
                            f"[Knowledge] OCR è¯†åˆ«æˆåŠŸï¼Œæ–‡å­—é•¿åº¦: {len(ocr_text)}, blocks: {len(ocr_result.get('blocks', []))}")
                except Exception as ocr_error:
                    logger.warning(f"[Knowledge] OCR è¯†åˆ«å¤±è´¥: {ocr_error}")

            # å¤„ç†æ–‡æ¡£
            processor = DocumentProcessor()
            documents = processor.process_file(file_path)

            # å¦‚æœæ˜¯å›¾ç‰‡ä¸”æ²¡æœ‰æ–‡æ¡£å†…å®¹ï¼Œä½†æœ‰ OCR ç»“æœï¼Œä½¿ç”¨ OCR å†…å®¹
            if not documents and ocr_text:
                from rag.document_processor import DocumentChunk
                documents = [DocumentChunk(
                    content=ocr_text,
                    metadata={
                        'source': os.path.basename(file_path),
                        'file_type': 'image',
                        'ocr': True
                    }
                )]
                logger.info(f"[Knowledge] ä½¿ç”¨ OCR å†…å®¹ä½œä¸ºæ–‡æ¡£")

            if not documents:
                return {
                    "type": "knowledge_add_document_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": False,
                    "error": "æ–‡æ¡£è§£æå¤±è´¥æˆ–æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ",
                }

            # åˆ†å—
            splitter = SmartTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = splitter.split_documents(documents)

            # è½¬æ¢ä¸ºå‘é‡å­˜å‚¨æ–‡æ¡£
            vectorstore = get_vectorstore()
            embedding_service = get_embedding_service()

            docs_to_add = []
            for chunk in chunks:
                doc = Document(
                    id=str(uuid.uuid4()),
                    content=chunk.content,
                    metadata=chunk.metadata,
                )
                docs_to_add.append(doc)

            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            count = await vectorstore.add_documents(
                knowledge_id=knowledge_id,
                documents=docs_to_add,
                embedding_service=embedding_service,
            )

            # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ–‡ä»¶è·¯å¾„ä¸­çš„æ–‡ä»¶å
            file_name = original_file_name or os.path.basename(file_path)

            return {
                "type": "knowledge_add_document_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "document": {
                    "id": str(uuid.uuid4()),
                    "knowledgeId": knowledge_id,
                    "fileName": file_name,
                    "filePath": file_path,
                    "fileType": file_ext,
                    "fileSize": os.path.getsize(file_path),
                    "chunkCount": count,
                    "ocrText": ocr_text,
                    "ocrBlocks": ocr_blocks,
                    "createdAt": int(time.time() * 1000),
                },
            }

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£é”™è¯¯: {e}")
            return {
                "type": "knowledge_add_document_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    async def _handle_knowledge_search(self, message: dict) -> dict:
        """å¤„ç†çŸ¥è¯†åº“æœç´¢è¯·æ±‚"""
        knowledge_id = message.get("knowledgeId")
        query = message.get("query", "")
        top_k = message.get("topK", 5)
        method = message.get("method", "hybrid")

        if not knowledge_id or not query:
            return {
                "type": "knowledge_search_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "results": [],
                "count": 0,
                "error": "çŸ¥è¯†åº“ ID å’ŒæŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
            }

        try:
            from rag.retriever import KnowledgeRetriever
            from rag.vectorstore import get_vectorstore
            from rag.embeddings import get_embedding_service

            vectorstore = get_vectorstore()
            embedding_service = get_embedding_service()

            retriever = KnowledgeRetriever(vectorstore, embedding_service)

            results = await retriever.retrieve(
                knowledge_id=knowledge_id,
                query=query,
                top_k=top_k,
                method=method,
            )

            # è½¬æ¢ç»“æœæ ¼å¼
            search_results = []
            for result in results:
                search_results.append({
                    "content": result.content,
                    "score": result.score,
                    "metadata": result.metadata,
                    "retrievalMethod": result.retrieval_method,
                })

            return {
                "type": "knowledge_search_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "results": search_results,
                "count": len(search_results),
            }

        except Exception as e:
            logger.error(f"æœç´¢çŸ¥è¯†åº“é”™è¯¯: {e}")
            return {
                "type": "knowledge_search_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "results": [],
                "count": 0,
                "error": str(e),
            }

    async def _handle_knowledge_list_documents(self, message: dict) -> dict:
        """å¤„ç†åˆ—å‡ºçŸ¥è¯†åº“æ–‡æ¡£è¯·æ±‚"""
        # è¿™ä¸ªæ–¹æ³•ä¸»è¦ä¾èµ– Electron ç«¯çš„æ•°æ®åº“ï¼Œè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨
        knowledge_id = message.get("knowledgeId")

        return {
            "type": "knowledge_list_documents_response",
            "id": message.get("id"),
            "timestamp": int(time.time() * 1000),
            "success": True,
            "documents": [],
            "count": 0,
        }

    # ==================== Memory è®°å¿†ç›¸å…³æ¶ˆæ¯å¤„ç† ====================

    async def _handle_memory_generate_summary(self, message: dict) -> dict:
        """
        å¤„ç†æ‘˜è¦ç”Ÿæˆè¯·æ±‚

        ä»å¯¹è¯æ¶ˆæ¯ä¸­ç”Ÿæˆæ‘˜è¦ï¼Œç”¨äºå¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†ã€‚
        """
        conversation_id = message.get("conversationId")
        messages = message.get("messages", [])
        model_id = message.get("modelId")

        if not conversation_id or not messages:
            return {
                "type": "memory_generate_summary_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘ conversationId æˆ– messages",
            }

        try:
            from memory_service import memory_service

            # ç”Ÿæˆæ‘˜è¦
            result = await memory_service.generate_summary(messages, model_id)

            if not result:
                return {
                    "type": "memory_generate_summary_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": False,
                    "error": "æ‘˜è¦ç”Ÿæˆå¤±è´¥",
                }

            logger.info(f"[Memory] ç”Ÿæˆæ‘˜è¦æˆåŠŸ: conversation_id={conversation_id}")

            return {
                "type": "memory_generate_summary_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "conversationId": conversation_id,
                "summary": result.get("summary", ""),
                "keyTopics": result.get("key_topics", []),
                "pendingTasks": result.get("pending_tasks", []),
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦é”™è¯¯: {e}")
            return {
                "type": "memory_generate_summary_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    async def _handle_memory_extract(self, message: dict) -> dict:
        """
        å¤„ç†è®°å¿†æå–è¯·æ±‚

        ä»å¯¹è¯ä¸­æå–ç”¨æˆ·åå¥½ã€é¡¹ç›®ä¸Šä¸‹æ–‡ã€ä»»åŠ¡è¿›åº¦ç­‰ä¿¡æ¯ã€‚
        """
        conversation_id = message.get("conversationId")
        messages = message.get("messages", [])
        model_id = message.get("modelId")

        if not messages:
            return {
                "type": "memory_extract_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘ messages",
            }

        try:
            from memory_service import memory_service

            # æå–è®°å¿†
            result = await memory_service.extract_memory(messages, model_id)

            if not result:
                return {
                    "type": "memory_extract_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": True,
                    "memories": {},
                }

            logger.info(f"[Memory] æå–è®°å¿†æˆåŠŸ: {list(result.keys())}")

            return {
                "type": "memory_extract_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "conversationId": conversation_id,
                "memories": result,
            }

        except Exception as e:
            logger.error(f"æå–è®°å¿†é”™è¯¯: {e}")
            return {
                "type": "memory_extract_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    # ==================== Web Crawl ç½‘é¡µé‡‡é›†å¤„ç† ====================

    async def _handle_web_crawl(self, message: dict) -> dict:
        """
        å¤„ç†ç½‘é¡µé‡‡é›†è¯·æ±‚

        æŠ“å–ç½‘é¡µå†…å®¹å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
        """
        url = message.get("url")
        knowledge_id = message.get("knowledgeId")
        title = message.get("title")
        chunk_size = message.get("chunkSize", 500)

        if not url:
            return {
                "type": "web_crawl_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘ URL",
            }

        if not knowledge_id:
            return {
                "type": "web_crawl_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘çŸ¥è¯†åº“ ID",
            }

        try:
            from agent.web_crawler import get_web_crawler

            logger.info(f"[WebCrawl] å¼€å§‹é‡‡é›†: {url} -> {knowledge_id}")

            crawler = get_web_crawler()
            result = await crawler.crawl_and_store(
                url=url,
                knowledge_id=knowledge_id,
                title=title,
                chunk_size=chunk_size,
            )

            if result.get("success"):
                logger.info(f"[WebCrawl] é‡‡é›†æˆåŠŸ: {result.get('title')}")
                return {
                    "type": "web_crawl_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": True,
                    "url": result.get("url"),
                    "title": result.get("title"),
                    "chunks": result.get("chunks"),
                    "knowledgeId": result.get("knowledge_id"),
                    "documentCount": result.get("document_count"),
                }
            else:
                return {
                    "type": "web_crawl_response",
                    "id": message.get("id"),
                    "timestamp": int(time.time() * 1000),
                    "success": False,
                    "error": result.get("error", "é‡‡é›†å¤±è´¥"),
                }

        except Exception as e:
            logger.error(f"[WebCrawl] é‡‡é›†é”™è¯¯: {e}")
            return {
                "type": "web_crawl_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    async def _handle_web_fetch(self, message: dict) -> dict:
        """
        å¤„ç†ç½‘é¡µå†…å®¹è·å–è¯·æ±‚

        ä»…è·å–ç½‘é¡µå†…å®¹ï¼Œä¸å…¥åº“ã€‚
        """
        url = message.get("url")
        max_length = message.get("maxLength", 5000)

        if not url:
            return {
                "type": "web_fetch_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": "ç¼ºå°‘ URL",
            }

        try:
            from agent.web_crawler import get_web_crawler

            logger.info(f"[WebFetch] è·å–å†…å®¹: {url}")

            crawler = get_web_crawler()
            crawler._max_length = max_length

            content = await crawler.fetch(url)

            logger.info(f"[WebFetch] è·å–æˆåŠŸ: {content.title}")

            return {
                "type": "web_fetch_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": True,
                "url": content.url,
                "title": content.title,
                "content": content.content,
            }

        except Exception as e:
            logger.error(f"[WebFetch] è·å–é”™è¯¯: {e}")
            return {
                "type": "web_fetch_response",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }

    # ==================== Ask é€šç”¨äº¤äº’æ¨¡å— ====================

    async def _handle_ask_response(self, message: dict) -> dict:
        """
        å¤„ç†ç”¨æˆ·å¯¹ Ask çš„å“åº”

        ç”¨æˆ·å“åº”é€šè¿‡ AskHandler å¤„ç†ï¼Œè®¾ç½®åˆ°å¯¹åº”çš„è¯¢é—®ä¼šè¯ä¸­ã€‚
        åç»­å¤„ç†é€šè¿‡ä¸Šä¸‹æ–‡è¿›è¡Œã€‚
        """
        try:
            success = await self.ask_handler.handle_response(message)
            return {
                "type": "ask_response_ack",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": success,
                "askId": message.get("askId"),
            }
        except Exception as e:
            logger.error(f"[Ask] å¤„ç†å“åº”é”™è¯¯: {e}")
            return {
                "type": "ask_response_ack",
                "id": message.get("id"),
                "timestamp": int(time.time() * 1000),
                "success": False,
                "error": str(e),
            }
